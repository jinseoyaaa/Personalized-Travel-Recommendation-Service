{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7922cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\art\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\art\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\art\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from NaverCrawling import NaverCrawling\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# 네이버 Search API의 ID와 Secret code 입력\n",
    "client_id = \"\"\n",
    "client_secret = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d886dbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API 호출 및 저장 중: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API 호출 및 서울 +강남구 여행에 대한 100개의 검색 데이터 저장 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API 호출 및 저장 중: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API 호출 및 서울 +강남구 가볼만한 곳에 대한 100개의 검색 데이터 저장 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API 호출 및 저장 중: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API 호출 및 서울 +강남구 관광지에 대한 100개의 검색 데이터 저장 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API 호출 및 저장 중: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API 호출 및 서울 +강남구 맛집에 대한 100개의 검색 데이터 저장 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API 호출 및 저장 중: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API 호출 및 서울 +강남구 명소에 대한 100개의 검색 데이터 저장 완료.\n",
      "Naver blog 링크의 수: 467\n",
      "\n",
      "467개의 링크에 대해서 크롤링 시작.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Crawling:   2%|█▎                                                                    | 9/467 [00:29<25:10,  3.30s/link]\n"
     ]
    }
   ],
   "source": [
    "# 크롤링 모듈\n",
    "# Class 호출\n",
    "crawler = NaverCrawling(client_id, client_secret)\n",
    "\n",
    "# 검색 단어 입력\n",
    "word = '강남구'\n",
    "search_words = [f\"서울 +{word} {category}\" for category in ['여행', '가볼만한 곳', '관광지', '맛집', '명소']]\n",
    "\n",
    "# 파일명 설정\n",
    "link_file = f\"./link/{word}_links.csv\"  # 블로그 링크가 저장될 파일\n",
    "search_count = 1  # 검색 횟수\n",
    "result_file = f\"./result/{word}_results.csv\"  # 크롤링 결과가 저장될 파일\n",
    "blacklist_file = \"blacklist.csv\"\n",
    "\n",
    "#for search_word in search_words:\n",
    "crawler.search(search_words, link_file, search_count, blacklist_file)\n",
    "crawler.crawling(link_file, result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25959746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 파일 불러오기\n",
    "df = pd.read_csv('search_list.csv') # 검색어 리스트, 각 지역명은 '+'기호로 반드시 포함\n",
    "blacklist_file = \"blacklist.csv\" # 이상한 블로그 제외(정치, 뉴스 스크랩, 지나친 광고)\n",
    "\n",
    "# Class 호출\n",
    "crawler = NaverCrawling(client_id, client_secret)\n",
    "\n",
    "# 각 단어에 대해 크롤링 작업 수행\n",
    "for word in df['word']:\n",
    "    # 검색 단어 설정\n",
    "    search_words = [f\"{word} {category}\" for category in ['여행', '가볼만한 곳', '관광지', '맛집', '명소']]\n",
    "    \n",
    "    # 링크 저장 파일명 설정\n",
    "    link_file = f\"./link/{word}_links.csv\"\n",
    "    \n",
    "    # 검색 횟수 설정\n",
    "    search_count = 1\n",
    "    \n",
    "    # 크롤링 결과 저장 파일명 설정\n",
    "    result_file = f\"./result/{word}_results.csv\"\n",
    "    \n",
    "    # 검색 및 링크 수집\n",
    "    crawler.search(search_words, link_file, search_count, blacklist_file)\n",
    "    \n",
    "    # 크롤링 수행\n",
    "    crawler.crawling(link_file, result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f879b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL 그룹을 나누어 CSV 파일로 저장\n",
    "def save_url_group(url_list, group_count):\n",
    "    for i in range(group_count):\n",
    "        group_urls = url_list[i::group_count]  # 그룹별로 URL 선택\n",
    "        with open(f'group_{i+1}.csv', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['url'])  # 헤더 행 추가\n",
    "            writer.writerows(group_urls)  # URL 데이터 작성\n",
    "\n",
    "# CSV 파일에서 URL 그룹 읽어오기\n",
    "def read_url_group(csv_file):\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # 헤더 행 건너뛰기\n",
    "        urls = [row[0] for row in reader]  # URL 데이터 읽어오기\n",
    "    return urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
