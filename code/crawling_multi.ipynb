{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e276fbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\art\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\art\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\art\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from NaverCrawling import NaverCrawling\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import concurrent.futures\n",
    "import glob\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "client_id = \"\"\n",
    "client_secret = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1be8b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class 호출\n",
    "crawler = NaverCrawling(client_id, client_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "665991c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dict contains fields not in fieldnames: 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m group_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# 데이터를 그룹으로 나누어 CSV 파일로 저장\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[43msave_data_to_csv_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_count\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 47\u001b[0m, in \u001b[0;36msave_data_to_csv_groups\u001b[1;34m(data, group_count)\u001b[0m\n\u001b[0;32m     45\u001b[0m writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(csvfile, fieldnames\u001b[38;5;241m=\u001b[39mfieldnames)\n\u001b[0;32m     46\u001b[0m writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[1;32m---> 47\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\csv.py:157\u001b[0m, in \u001b[0;36mDictWriter.writerows\u001b[1;34m(self, rowdicts)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriterows\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdicts):\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterows\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowdicts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\csv.py:149\u001b[0m, in \u001b[0;36mDictWriter._dict_to_list\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    147\u001b[0m     wrong_fields \u001b[38;5;241m=\u001b[39m rowdict\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrong_fields:\n\u001b[1;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict contains fields not in fieldnames: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m                          \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wrong_fields]))\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (rowdict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestval) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames)\n",
      "\u001b[1;31mValueError\u001b[0m: dict contains fields not in fieldnames: 'content'"
     ]
    }
   ],
   "source": [
    "# 폴더 경로 설정\n",
    "folder_path = './Test/'\n",
    "\n",
    "# 폴더 내의 파일 리스트 가져오기\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "# 첫 번째 파일 이름 추출\n",
    "first_file_name = file_names[0].split('.')[0].split()[0]\n",
    "\n",
    "# 결과 파일 경로 설정\n",
    "result_file = f'./Test/{first_file_name}.csv'\n",
    "\n",
    "# CSV 파일에서 데이터를 모두 가져오는 함수\n",
    "def get_all_data_from_csv(folder_path):\n",
    "    \"\"\"\n",
    "    폴더 내의 모든 CSV 파일에서 데이터를 추출하는 함수입니다.\n",
    "    :param folder_path: CSV 파일이 있는 폴더 경로\n",
    "    :return: 추출된 데이터를 담은 리스트\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    csv_files = glob.glob(folder_path + '/*.csv')\n",
    "    for csv_file in csv_files:\n",
    "        with open(csv_file, 'r', encoding='utf-8-sig') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                data.append(row)\n",
    "    return data\n",
    "\n",
    "# 데이터를 그룹으로 나누어 CSV 파일로 저장하는 함수\n",
    "def save_data_to_csv_groups(data, group_count):\n",
    "    \"\"\"\n",
    "    데이터를 그룹으로 나누어 CSV 파일로 저장하는 함수입니다.\n",
    "    :param data: 저장할 데이터를 담은 리스트\n",
    "    :param group_count: 그룹 개수\n",
    "    \"\"\"\n",
    "    for i in range(group_count):\n",
    "        group_data = data[i::group_count]\n",
    "        with open(f'./TEST/group_{i+1}.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "            fieldnames = data[0].keys()\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(group_data)\n",
    "\n",
    "def delete_rows(csv_file, links):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df[~df['link'].isin(links)]\n",
    "    df.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 폴더 내의 모든 CSV 파일에서 데이터 추출\n",
    "data = get_all_data_from_csv(folder_path)\n",
    "\n",
    "# 그룹 개수 설정\n",
    "group_count = 1\n",
    "\n",
    "# 데이터를 그룹으로 나누어 CSV 파일로 저장\n",
    "save_data_to_csv_groups(data, group_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc9a1ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling: 100%|████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.48s/link]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링 완료: 2 링크에 대한 본문 내용 추출됨.\n",
      "Total execution time: 0시 0분 13초\n"
     ]
    }
   ],
   "source": [
    "min_delay = 1\n",
    "max_delay = 5\n",
    "start_time = time.time()\n",
    "# 병렬로 크롤링 작업 실행\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     futures = []\n",
    "#     for i in range(group_count):\n",
    "#         csv_file = f'./TEST/group_{i+1}.csv'\n",
    "#         delay = random.uniform(min_delay, max_delay)  # 요청 간격 랜덤 설정\n",
    "#         user_agent = random.choice(user_agents)  # 사용자 에이전트 랜덤 선택\n",
    "#         time.sleep(delay)  # 요청 간격 대기\n",
    "#         futures.append(executor.submit(crawler.crawl_links, csv_file, result_file, user_agent))\n",
    "    \n",
    "#     for future in as_completed(futures):\n",
    "#         pass\n",
    "\n",
    "# 병렬 크롤링\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for i in range(group_count):\n",
    "        csv_file = f'./TEST/group_{i+1}.csv'\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "        futures.append(executor.submit(crawler.crawl_links, csv_file, result_file, i))\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        pass   \n",
    "\n",
    "# 크롤링 결과에서 필요한 열 추출\n",
    "#crawler.extract_columns(csv_file)\n",
    "\n",
    "# 총 실행 시간 계산\n",
    "execution_time = time.time() - start_time\n",
    "minutes, seconds = divmod(execution_time, 60)\n",
    "hours, minutes = divmod(minutes, 60)\n",
    "\n",
    "# 총 실행 시간을 시, 분, 초로 변환하여 출력\n",
    "print(f\"Total execution time: {int(hours)}시 {int(minutes)}분 {int(seconds)}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6aa8c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 정제 시작... 불필요한 열 제거 중...\n",
      "데이터 정제 및 불필요한 열 제거 완료\n"
     ]
    }
   ],
   "source": [
    "crawler.extract_columns(result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d692871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./TEST/test.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_file = './TEST/test.csv'\n",
    "result_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8426dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(result_file, encoding='utf-8-sig')\n",
    "df = df[df['content'].str.strip() != '']\n",
    "word_list = '개인사업자|행정구역'\n",
    "df = df[~df['content'].str.contains(word_list)]\n",
    "\n",
    "df.to_csv(result_file+'2', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08b4806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = './link'\n",
    "# csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# for i, file in enumerate(csv_files, start=1):\n",
    "#     file_path = os.path.join(folder_path, file)\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     # 파일 이름\n",
    "#     file_name = os.path.splitext(file)[0]\n",
    "    \n",
    "#     # \"_link\" 제거한 새로운 파일 이름\n",
    "#     new_file_name = file_name.replace('_links', '')\n",
    "    \n",
    "#     # \"시도\"와 \"시군구\" 컬럼에 입력\n",
    "#     sido, sigungu = new_file_name.split(' ', 1)\n",
    "#     df['시도'] = sido\n",
    "#     df['시군구'] = sigungu\n",
    "    \n",
    "#     # 변경된 DataFrame을 새로운 파일로 저장\n",
    "#     new_file_path = os.path.join(folder_path, f'{new_file_name}.csv')\n",
    "#     df.to_csv(new_file_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "#     # 기존 파일 삭제\n",
    "#     os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26134d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 현재 폴더의 파일 목록 가져오기\n",
    "# current_path = \"./link\"  # 현재 폴더의 경로 지정\n",
    "# file_list = [file for file in os.listdir(current_path) if os.path.isfile(os.path.join(current_path, file))]\n",
    "\n",
    "# # 확장자를 제외한 파일명 추출\n",
    "# file_names = [os.path.splitext(file)[0] for file in file_list]\n",
    "\n",
    "# # CODE.csv 파일 불러오기\n",
    "# code_df = pd.read_csv(\"CODE.csv\", encoding='cp949')  # CODE.csv 파일 경로를 지정해주세요\n",
    "\n",
    "# # \"시도 시군구\"와 일치하는 파일에 \"code\" 열 추가하여 \"법정동코드\" 값 할당\n",
    "# for file_name in file_names:\n",
    "#     matching_rows = code_df[code_df[\"시도 시군구\"] == file_name]\n",
    "#     if not matching_rows.empty:\n",
    "#         file_path = os.path.join(current_path, file_name + \".csv\")\n",
    "#         df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "#         df[\"code\"] = matching_rows.iloc[0][\"법정동코드\"]\n",
    "#         df.to_csv(file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4be77893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./TEST/제주특별자치도.csv', encoding='utf-8-sig')\n",
    "# # \"시군구\" 열 값에 따라 \"code\" 열 값 설정\n",
    "# df['code'] = df['시군구'].map({'제주시': '50110', '서귀포시': '50130'})\n",
    "\n",
    "# # 결과를 파일에 저장\n",
    "# df.to_csv('제주특별자치도.csv', index=False, encoding='utf-8-sig')\n",
    "# #crawler.extract_columns('./TEST/제주특별자치도.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b7437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 폴더 내 모든 CSV 파일에서 'link' 열의 값만을 리스트로 추출\n",
    "# def get_all_data_from_csv(folder_path):\n",
    "#     urls = []\n",
    "#     csv_files = glob.glob(folder_path + '/*.csv')\n",
    "#     for csv_file in csv_files:\n",
    "#         with open(csv_file, 'r', encoding='utf-8-sig') as csvfile:\n",
    "#             reader = csv.DictReader(csvfile)\n",
    "#             for row in reader:\n",
    "#                 urls.append(row['link'])\n",
    "#     return urls\n",
    "\n",
    "# # 각 그룹별로 병렬 크롤링 작업 실행\n",
    "# # csv 파일을 읽어 링크를 crawler.crawling에게 전달.\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     # 각 그룹의 CSV 파일을 읽어와서 크롤링 작업 수행\n",
    "#     futures = []\n",
    "#     for i in range(group_count):\n",
    "#         csv_file = f'./TEST/group_{i+1}.csv'\n",
    "#         group_urls = read_url_group(csv_file)\n",
    "#         futures.extend(executor.submit(crawler.crawling, link, result_file) for link in group_urls)\n",
    "\n",
    "#     # 각 크롤링 작업이 완료될 때까지 대기\n",
    "#     for future in concurrent.futures.as_completed(futures):\n",
    "#         # 크롤링 작업의 결과나 예외를 가져올 수 있음\n",
    "#         # result = future.result()\n",
    "#         pass\n",
    "    \n",
    "# get_all_data_from_csv에서 가져온 딕셔너리가 담긴 리스트를 csv로 변환\n",
    "# def save_url_group(url_list, group_count):\n",
    "#     for i in range(group_count):\n",
    "#         group_urls = url_list[i::group_count]  # 그룹별로 URL 선택\n",
    "#         with open(f'./TEST/group_{i+1}.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "#             writer = csv.writer(csvfile)\n",
    "#             writer.writerow(['link'])  # 헤더 행 추가\n",
    "#             for url in group_urls:\n",
    "#                 writer.writerow([url])  # URL 데이터 작성"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
